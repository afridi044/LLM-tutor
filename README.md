# ğŸ“ Biology Tutor Chatbot - Bryophyta & Pteridophyta

An intelligent chatbot powered by advanced RAG (Retrieval-Augmented Generation) for answering questions from a Bangla biology textbook.

## âœ¨ Features

- ğŸ” **Hybrid Retrieval**: Combines dense (FAISS) and sparse (BM25) search with Reciprocal Rank Fusion
- ğŸŒ **Multi-language**: Supports Bangla, English, and Banglish queries
- ğŸ“š **Source Citations**: Shows relevant textbook page references
- ğŸ’¡ **Educational**: Provides detailed explanations with examples
- âš¡ **Fast**: Pre-computed indices for instant responses
- ğŸ¨ **Beautiful UI**: Clean, intuitive Streamlit interface

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Set Up API Key

Create a `.streamlit/secrets.toml` file:

```toml
GEMINI_API_KEY = "your-gemini-api-key-here"
```

Or set environment variable:

```bash
# Windows PowerShell
$env:GEMINI_API_KEY="your-key-here"

# Linux/Mac
export GEMINI_API_KEY="your-key-here"
```

Get your API key from: https://makersuite.google.com/app/apikey

### 3. Preprocess Data (One-time)

Place your PDF (`BrTr_ocr.pdf`) in the project folder, then run:

```bash
python preprocess.py
```

This will:
- Extract and clean text from PDF
- Create semantic chunks
- Generate embeddings
- Build search indices
- Save everything to `data/` folder

**Expected output:**
```
âœ¨ PREPROCESSING COMPLETE!
ğŸ“ Data saved in: C:\path\to\data
ğŸ“Š Total file size: XX.XX MB
```

### 4. Run the Chatbot

```bash
streamlit run app.py
```

Open your browser at `http://localhost:8501`

## ğŸ“ Project Structure

```
LLM tutor/
â”œâ”€â”€ app.py                 # Streamlit chat interface
â”œâ”€â”€ rag_system.py          # Core RAG implementation
â”œâ”€â”€ config.py              # Configuration settings
â”œâ”€â”€ preprocess.py          # Data preprocessing script
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ BrTr_ocr.pdf          # Your textbook PDF
â””â”€â”€ data/                  # Generated indices (after preprocessing)
    â”œâ”€â”€ chunks.json        # Text chunks with metadata
    â”œâ”€â”€ embeddings.npy     # Dense embeddings
    â”œâ”€â”€ faiss_index.bin    # FAISS vector index
    â””â”€â”€ bm25_data.pkl      # BM25 sparse index
```

## ğŸ¯ Usage Examples

**Questions you can ask:**

- `à¦°à¦¿à¦•à¦¸à¦¿à¦¯à¦¼à¦¾à¦° à¦¬à§ˆà¦¶à¦¿à¦·à§à¦Ÿà§à¦¯ à¦•à§€?`
- `What is the difference between bryophytes and pteridophytes?`
- `à¦®à¦¸ à¦‰à¦¦à§à¦­à¦¿à¦¦à§‡à¦° à¦œà§€à¦¬à¦¨à¦šà¦•à§à¦° à¦¬à§à¦¯à¦¾à¦–à§à¦¯à¦¾ à¦•à¦°à§‹`
- `riccia er shonaktokari boishisto gulo bolo` (Banglish)

## âš™ï¸ Configuration

Edit `config.py` to customize:

- **Models**: Change `CHAT_MODEL` or `EMBEDDING_MODEL`
- **Retrieval**: Adjust `TOP_K_RETRIEVE`, `CHUNK_SIZE`, etc.
- **UI**: Modify `APP_TITLE`, colors, welcome message
- **Performance**: Enable caching, rate limiting

## ğŸŒ Deployment

### Option 1: Streamlit Community Cloud (Free)

1. Push code to GitHub
2. Go to https://share.streamlit.io
3. Deploy from your repository
4. Add `GEMINI_API_KEY` in Secrets management

**Important**: Upload the `data/` folder to your repo (or use GitHub LFS for large files)

### Option 2: Hugging Face Spaces (Free)

1. Create a new Space at https://huggingface.co/spaces
2. Choose "Streamlit" as SDK
3. Upload all files including `data/` folder
4. Add `GEMINI_API_KEY` in Settings â†’ Secrets

### Option 3: Local Network

```bash
streamlit run app.py --server.address 0.0.0.0 --server.port 8501
```

Access from other devices: `http://your-ip:8501`

### Option 4: Docker (Production)

Create `Dockerfile`:

```dockerfile
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 8501

CMD ["streamlit", "run", "app.py", "--server.address", "0.0.0.0"]
```

Build and run:

```bash
docker build -t biology-tutor .
docker run -p 8501:8501 -e GEMINI_API_KEY=your-key biology-tutor
```

## ğŸ”§ Troubleshooting

### "Data files not found"
- Run `python preprocess.py` first
- Ensure `data/` folder exists with all 4 files

### "API key missing"
- Set `GEMINI_API_KEY` in `.streamlit/secrets.toml` or environment
- Check key is valid at https://makersuite.google.com

### Slow performance
- Reduce `TOP_K_RETRIEVE` in sidebar (try 5 instead of 10)
- Use `gemini-2.0-flash-exp` instead of `gemini-1.5-pro`
- Ensure preprocessing completed successfully

### Out of memory
- Reduce `CHUNK_SIZE` in `config.py`
- Use smaller embedding model (e.g., `paraphrase-multilingual-MiniLM-L12-v2`)

## ğŸ“Š System Requirements

- **RAM**: 2GB minimum (4GB recommended)
- **Disk**: ~500MB for models + data
- **Python**: 3.9+
- **Internet**: Required for Gemini API calls

## ğŸ”’ Security Notes

- Never commit `.streamlit/secrets.toml` to GitHub
- Add to `.gitignore`:
  ```
  .streamlit/secrets.toml
  .env
  ```
- Use environment variables in production
- Consider rate limiting for public deployments

## ğŸ“ˆ Performance Tips

1. **Pre-compute everything**: Run `preprocess.py` offline
2. **Use caching**: Streamlit caches the RAG system automatically
3. **Optimize queries**: Shorter, focused questions work best
4. **Batch processing**: Use `data/` folder with pre-built indices
5. **Model selection**: `gemini-2.0-flash-exp` is 10x faster than `gemini-1.5-pro`

## ğŸ¤ Contributing

Feel free to:
- Add more textbooks (modify `preprocess.py`)
- Improve chunking strategies (`rag_system.py`)
- Enhance UI (`app.py`)
- Add new features (export chat, PDF viewer, etc.)

## ğŸ“ License

This project is for educational purposes. Ensure you have rights to the textbook content.

## ğŸ†˜ Support

Issues? Questions?
- Check `config.py` for settings
- Review logs in terminal
- Ensure all dependencies installed
- Verify API key is valid

## ğŸ“ How It Works

1. **Preprocessing** (one-time):
   - PDF â†’ Text extraction â†’ OCR cleaning
   - Text â†’ Semantic chunks (800 chars)
   - Chunks â†’ Dense embeddings (768-dim vectors)
   - Build FAISS index (fast similarity search)
   - Build BM25 index (keyword search)

2. **Query time** (real-time):
   - User question â†’ Query expansion (translation, paraphrasing, HyDE)
   - Parallel retrieval: Dense (FAISS) + Sparse (BM25)
   - Reciprocal Rank Fusion â†’ combine results
   - Cross-encoder reranking â†’ final top-K
   - Context assembly â†’ LLM generation (Gemini)
   - Answer + source citations

## ğŸ”¬ RAG Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User Query  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Query Expansion  â”‚ (Gemini)
â”‚ - Translation    â”‚
â”‚ - Paraphrasing   â”‚
â”‚ - HyDE           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Dense  â”‚ â”‚ Sparse â”‚
â”‚ FAISS  â”‚ â”‚  BM25  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚         â”‚
     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   RRF    â”‚
    â”‚ Fusion   â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
          â”‚
          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Rerankingâ”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
          â”‚
          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Context  â”‚
    â”‚ Assembly â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
          â”‚
          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Gemini  â”‚
    â”‚   LLM    â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
          â”‚
          â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Answer  â”‚
    â”‚ +Sources â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**Built with â¤ï¸ using Streamlit, Gemini, and state-of-the-art RAG techniques**
